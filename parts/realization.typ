= Реализация

Создание инструмента для применения предложенного метода извлечения поведенческих моделей библиотек заключается в реализации каждой из его частей, разобранных ранее, и их связи между собой и пользователем. В данном разделе приведена общая архитектура инструмента и детали реализации его модулей, включая различные инженерные проблемы и пути их решения.  

== Общая архитектура

Реализованный инструмент может быть использован как консольное приложение или как библиотека, с помощью которой пользователь сможет реализовывать свои сценарии анализа или пользоваться отдельными её частями. Основой инструмента являются несколько пакетов:
+ analysis -- пакет, содержащий реализацию извлечения трасс с помощью статического и динамического анализа, а именно построение ICFG, points-to анализ, фаззинг и сбор точек входа в программу
+ inference -- пакет, обеспечивающий обработку трасс и восстановление из них поведенческих моделей
+ repository -- пакет, реализующий получение репозиториев и работу с ними
+ storage -- пакет, отвечающий за работу с БД
+ workflow -- пакет, связывающий все реализованные возможности между собой с помощью определенных сценариев работы. Стоит отметить, что по сути пакет содержит примеры использования реализованных пакетов и не требует детального рассмотрения

== Пакет repository

Данный пакет решает задачи поиска, получение и сборки проектов из GitHub и Maven Central Repository. Также в рамках этого пакета реализована возможность запуска тестов, имеющихся в репозитории. Рассмотрим каждую из задач подробнее.

=== Получение проектов с GitHub

Не смотря на то, что GitHub предоставляет открытый API для поиска по коду, при разработке возникли несколько проблем:
+ Поиск происходит по файлам, соответственно, сами репозитории могут повторяться. Необходимо это учитывать, чтобы не тратить лишнее время на получение и анализ уже рассмотренного ранее проекта.
+ Один поисковой запрос может найти не более 1000 результатов, что указано в документации#footnote("https://docs.github.com/en/rest/search/search?apiVersion=2022-11-28#about-search"). С учетом повторения репозиториев, этого может быть недостаточно для автоматической работы длительное время с целью изучить большое количество проектов. При этом сторонние системы поиска по коду, например SearchCode, имеют более сильные ограничения.
+ Частые запросы к API приводят к достижению secondary rate limit#footnote("https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api?apiVersion=2022-11-28#about-secondary-rate-limits"). При этом, основного лимита достаточно для выполнения запросов. Каким-то образом необходимо избежать введения искусственных задержек.

Для каждой из этих проблем были реализованы следующие решения:
+ Для исключения повторений следует хранить авторов и названия проанализированных репозитории в БД. Это также будет полезно для остановки и возобновлений поисковых сессий. Чтобы не обращаться каждый раз в БД для проверки репозитория на повторение, следует реализовать кэш.
+ Чтобы обойти лимит результатов поиска, реализован адаптивный фильтр по размеру файла. Способ фильтрации предлагает сам GitHub. Таким образом, мы можем задать диапазон от 0 до максимального размера файла, который нас интересует, и проходить его определенными шагами, например, изначально 100 байт. Если количество результатов поиска находится между нулем и 1000, то не меняем шаг. Если количество результатов равно нулю, то мы увеличиваем шаг в два раза, а если максимальному значению -- уменьшаем шаг в два раза.
+ Во избежание secondary rate limit применяется ленивое получение репозиториев и их последовательный анализ, реализованное с помощью наследования Sequence в языке Kotlin. Каждый запрос к API возвращает набор файлов, из которых извлекаются уникальные репозитории, и до тех пор, пока все они не будут проанализированы, API использоваться не будет. Таким образом создается естественная задержка, позволяющая избегать лимита. Если он все же был достигнут (например, подряд выполнялись несколько уменьшений или увеличений размера шага), то используется sleep перед очередной попыткой запроса.

Для поиска и получения проектов с GitHub необходимо предварительно получить и указать свой токен для API в конфигурации инструмента, а на вход подать поисковой запрос и шаблон имени файла. Это необходимо, так как можно искать по импорту среди:
- .java файлов (path:\*\*\/\*.java)
- build.gradle (path:\*\*\/build.gradle)
- build.gradle.kts (path:\*\*\/build.gradle.kts или path:\*\*\/build.gradle\* для всех вариаций)
- pom.xml (path:\*\*\/pom.xml)

Для работы с удаленными GitHub репозиториями реализован класс GhRemoteRepository, позволяющий получать доступные артефакты и исходный код всего проекта:
+ Определяется наличие выпущенной версии
+ Проверяется наличие JAR файла соответствующего версии
+ Проверяется наличие исходного кода соответствующего версии
+ Если найдены, скачиваются JAR файл и архив с кодом версии, иначе выполняется клонирование с помощью git

=== Получение проектов с Maven Central

Веб-сайт Sonatype#footnote("https://central.sonatype.com/") предлагает возможность получения зависимых библиотек от выбранной. Однако публичного API для этого не предоставлено. С помощью инструментов разработчика в браузере было найдено внутреннее API, реализующее необходимую в рамках работы задачу. Способ нахождения и структура тела запроса представлена на @internal_api_struct. Чтобы исключить вероятность блокировки при использовании данного API, было направлено электронное письмо в поддержку Sonatype. В ответе говорилось, что допустимо использовать внутренний API, если нет намерений просканировать весь Maven Central, что в рамках выполняемой работы нас устраивает. 

#figure(
  image("../img/internal_api_structure.png", height: 23%),
  caption: "Поиск необходимого API"
) <internal_api_struct>

Для поиска проектов следует указать группу, название и версию библиотеки.

Получение найденных проектов осуществляется с помощью специально созданного build.gradle файла на одну зависимость, позволяющего подставлять в него библиотеку для анализа и выполнять задачу разрешения зависимостей. Таким образом происходит автоматическое получение и сборка библиотеки для анализа и всех её зависимостей в заданную директорию.

Как уже было упомянуто в разделе, посвященному проектированию, получение исходного кода из Maven Central бессмысленно, так как он не содержит тестов, а сборка выполнена автоматически при получении проекта с помощью gradle.

=== Сборка проектов <building_prj>

Проекты из Maven Central собираются автоматически из-за выбранного метода их получения. Что касается проектов с GitHub, то даже при наличии JAR файла в репозитории, он может не содержать зависимости, а следовательно, он может не запуститься. Также необходима компиляция тестов, если они есть в проекте. 

Для этого требуется в первую очередь определить, какая система сборки используется в проекте. Это делается с помощью обхода файлов и поиска характерых файлов сборки: build.gradle, build.gradle.kts, pom.xml. Если ни один из файлов не обнаружен в корне проекта, то выполняется обход вложенных директорий, так как репозиторий может иметь нестандартные вложения или состоять из нескольких модулей.

Если проект использует Gradle, то вся работа с ним может быть выполнена с помощью Gradle Tooling API. В зависимости от конфигурации инструмента версия Gradle может быть установлена автоматически для каждого проекта, зафиксирована или может быть использован уже установленный на машине сборщик.

Maven не предполагает подобного API, поэтому в зависимости от конфигурации используется сборщик, к которому указан путь или из переменных окружения.

Класс локальных репозиториев предусматривает методы сбора трасс из файлов, которые создаются в корне проекта как результат запуска инструментированного кода.

== Пакет analysis

Данный пакет содержит реализацию статического анализа, инструментацию и запуск фаззинга, в совокупности предоставляя возможности для извлечения трасс вызовов библиотеки с применением статических и динамических подходов.

=== Сбор точек входа в программу

Как для статического извлечения трасс, так и для фаззинга необходимо предварительно получить возможные точки в программу. Помимо main метода, это могут быть:
- Обработчики событий. Например методы, отвечающие за обработку запросов
- Конструкторы
- Конкретные известные пользователю методы

Чтобы иметь гибкость вопросе выбора входных точек в программу, реализован механизм фильтрации для методов. Фильтры пишутся в YAML формате, в процессе работы инструмента десериализуются и используются для определения входных точек при обходе всех методов в программе. Фильтр содержит следующие поля:
+ methodAnnotation -- множество аннотаций метода, доступных в runtime
+ methodName -- регулярное выражение для названия метода
+ сlassAnnotation -- множество аннотаций класса, доступных в runtime
+ className -- регулярное выражение для названия класса
+ kind -- вид метода, может быть модификатором доступа, конструктором (init) или статическим инициализатором (clinit)
+ args -- список типов аргументов метода в их оригинальном порядке
+ returnType -- возвращаемый тип

Если значения равны null, то параметр фильтра игнорируется. Пример фильтра для поиска публичных методов с названием, начинающимся на main, принимающих в себя массив строк, приведен на @filter_entry_example. 

#figure(
  ```yaml
methodTags: null
methodName: "main.*"
classTags: null
className: null
kind: "public"
args: ["java.lang.String[]"]
returnType: null
```,
caption: "Пример фильтра в yaml формате"
) <filter_entry_example>

=== Статическое извлечение трасс

Извлечение трасс с применением статического анализа состоит из нескольких основных шагов:
+ Получить точки входа в программу. Эта задача подробно разобрана чуть выше.
+ Построить ICFG. Решается использованием стандартного API Soot, однако построение межпроцедурного графа потока управления требует запуска Soot в режиме "whole-program".
+ Выполнить обход ICFG для каждой точки входа, собирая вызовы и группируя их посредством применения points-to анализа к объектам. Рассмотрим данную задачу отдельно, так как она требует разработки определенного алгоритма обхода.

Для обхода ICFG и выполнения всех необходимых для извлечения трассы действий следует реализовать SceneTransormer и добавить его PackageManager. Именно созданный SceneTransormer будет определять проводимый анализ.

ICFG представляет собой все возможные пути выполнения программы и, очевидно, в среднем содержит множество циклов и ветвлений. Так как нас интересуют именно трассы исполнения, их длинна может быть бесконечной и ее следует ограничивать. Для этого были введены два параметра:
- Глубина обхода. Изначально устанавливается некоторое число, которое уменьшается каждый раз, когда мы уходим в реализацию вызова. Когда глубина равна 0, обход не может попасть в реализацию какого-либо вызова в рассматриваемом методе и обязан пройти его до конца. Возвращаясь на уровень выше, глубина инкрементируется.
- Длина. Задает лимит по количеству вершин графа в рамках обхода.

Извлечение трасс из ICFG представляет собой рекурсивный обход в глубину. Псевдокод алгоритма приведен на @graph_traverse_alg. Каждое ветвление имеет собственный набор трасс, при этом оно содержит общую часть с трассами, которые будут собраны для других веток (имеются ввиду ветки в одной и той же точке программы). Для того, чтобы не копировать наборы трасс, в методе фиксируется, какие из них были дополнены на данной итерации. В конце каждого метода, после выполнения рекурсивных вызовов, с помощью сохраненных индексов трассы приводятся в свое изначально состояние, которое было до ветвления. Сами трассы сохраняются в БД по достижению ограничений на обход или обходу всей программы. Для обхода в глубину самих вызовов в специальном стеке сохраняется точка в программе, к которой необходимо вернуться после рассмотрения метода до конца. Когда вызываемый метод рассмотрен, точка возврата снимается со стека.

Для поиска вызовов конкретной библиотеки выполняется сравнение с указанным префиксом названий пакетов. Обнаружив вызов, необходимо применить points-to анализ для объекта вызова и объектов из уже накопленных трасс, получив множества переменных, которые могут указывать на тот же объект. Если полученные множества пересекаются, то переменные могут указывать на один и тот же объект, и вызов помещается в соответствующую трассу, иначе создается новая. Таким образом, через points-to выражается alias анализ.

При обходе перебираются все возможные в рамках наложенных ограничений по длине и глубине комбинации ветвлений. Благодаря этому возможно получение высокого покрытия кода и сбора содержательных трасс вызовов. Собранные трассы представляют собой последовательность из MethodData, содержание которой можно увидеть в @dynamic_trace_example как часть динамически получаемой трассы.

#figure(
  ```fun graphTraverseLib(
        startPoint: Unit,
        isMainMethod: Boolean,
        ttl: Int = configuration.traversJumps,
        depth: Int = configuration.traversDepth
    ) {
        val currentSuccessors = icfg.getSuccsOf(startPoint)
        if (currentSuccessors.size == 0 || ttl <= 0) {
            if (ttl <= 0 || isMainMethod) {
                save(extracted)
            } else {
                val succInfo = continueStack.removeLast()
                graphTraverseLib(succInfo.first, succInfo.second, ttl - 1, depth + 1)
                continueStack.add(succInfo)
            }
        } else {
            for (succ in currentSuccessors) {
                var method: SootMethod? = null
                var continueAdded = false
                var klass: String? = null
                var indexesOfChangedTraces: List<Int>? = null
                    if (succ is JInvokeStmt || succ is JAssignStmt) {
                        succ as AbstractStmt
                        if (succ.invokeExpr.method.declaringClass in Scene.v().applicationClasses)
                            method = succ.invokeExpr.method
                        if (method?.foundLib(lib)) {
                            klass = method.declaringClass.toString()
                            if (extracted[klass] == null) extracted[klass] = mutableListOf()
                            indexesOfChangedTraces = saveInvokeToTrace(succ.invokeExpr, extracted[klass])
                        }
                    }
                if (method != null && depth > 0) {
                    continueAdded = continueStack.add(Pair(succ, isMainMethod))
                    icfg.getStartPointsOf(method).forEach { methodStart ->
                        graphTraverseLib(methodStart, false, ttl - 1, depth - 1)
                    }
                } else graphTraverseLib(succ, isMainMethod, ttl - 1, depth)

                if (indexesOfChangedTraces != null) resetTraces(indexesOfChangedTraces, extracted[klass])
                if (continueAdded) continueStack.removeLast()
            }
        }
    }```,
  caption: "Псевдокод алгоритма обхода"
) <graph_traverse_alg>


=== Динамическое извлечение трасс

Динамическое извлечение трасс в первую очередь требует инструментации. Она реализована с помощью создания собственного SceneTransormer в Soot и заключается во вставке кода после вызовов библиотеки. Вызовы библиотеки определяются тем же способом, что и при статическом анализе. Вставленный код собирает следующие данные:
+ Identity hash code объекта вызова
+ Время вызова в наносекундах
+ Uuid запуска
+ Название метода
+ Аргументы 
+ Возвращаемый тип
+ Класс метода

Для инструментации реализован вспомогательный класс, отвечающий за запись собранной информации в файл, который подкладывается в JAR. Файлы создаются для каждого потока, чтобы избежать конфликтов при многопоточной работе. Uuid необходим, чтобы различать попытки запуска и исключить совпадение identity hash code в рамках разных рабочих сессий. Записи в файле представляют собой сериализованные объекты InvokeData и MethodData, которые успешно десериализуются при сборе трасс и группируются по identity hash code и uuid.

Soot при инструментации JAR файла теряет директорию META-INF, влияющую на его запуск. Не смотря на то, что тесты не используют JAR, а фаззинг требует на вход путь к конкретному методу, это учтено и реализован механизм сохранения изначальной мета-информации и добавления её в инструментированный JAR. Это полезно для пользовательских экспериментов по извлечению трасс, позволяя сохранять JAR действительно исполняемым. 

Для получения трасс необходимо выполнить запуск тестов или фаззинг. Для запуска тестов используется Gradle Tooling API или предустановленный Maven тем же образом, что описан в @building_prj. При вызове задачи явно исключается сборка проекта, чтобы не потерять результаты инструментирования.

Для фаззинга автоматически при первом запуске происходит получение архива с исполняемыми файлами Jazzer под целевую операционную систему (в дальнейшем проверяется наличие исполняемых файлов). Сам фаззер вызывается как сторонний процесс для каждой точки входа и имеет следующие опции:
- -runs=N. Количество запусков указанного метода, где N задается пользователем. Если N = 0, то ограничение отсутствует.
- -max_total_time=N. Длительность работы фаззера в секундах, где N задается пользователем. Для бесконечной работы опция не указывается.
- \-\-keep_going=0. Игнорирует найденные фаззером ошибки. Эта опция нужна, так как мы заинтересованы в трассах, а не в тестировании программы. На самом деле вместо нуля можно указать любое другое число, означающее количество игнорируемых ошибок.
- \-\-autofuzz=qualified.reference.to.Class::method. Указывает цель для фаззинга.

Пример получаемой трассы для поиска вызовов класса java.io.File приведен в @dynamic_trace_example.

#figure(
  ```txt
{"methodData":{"name":"<init>","args":["java.lang.String"],"returnType":"void","isStatic":false,"klass":"java.io.File"},
"iHash":"443942537","date":"170309763...","uuid":"6b4..."}
{"methodData":{"name":"<init>","args":["java.lang.String"],"returnType":"void","isStatic":false,"klass":"java.io.File"},
"iHash":"1243171897","date":"170309805...","uuid":"6b4..."}
{"methodData":{"name":"exists","args":[],"returnType":"boolean","isStatic":false,"klass":"java.io.File"},
"iHash":"1243171897","date":"170309805...","uuid":"6b4..."}
{"methodData":{"name":"createNewFile","args":[],"returnType":"boolean","isStatic":false,"klass":"java.io.File"},
"iHash":"1243171897","date":"170309806...","uuid":"6b4..."}
{"methodData":{"name":"toPath","args":[],"returnType":"java.nio.file.Path","isStatic":false,"klass":"java.io.File"},
"iHash":"1243171897","date":"170309807...","uuid":"6b4..."}
{"methodData":{"name":"<init>","args":["java.lang.String"],"returnType":"void","isStatic":false,"klass":"java.io.File"},
"iHash":"1376151044","date":"170309807...","uuid":"6b4..."}
{"methodData":{"name":"delete","args":[],"returnType":"boolean","isStatic":false,"klass":"java.io.File"},
"iHash":"1376151044","date":"170309809...","uuid":"6b4..."}
{"methodData":{"name":"createNewFile","args":[],"returnType":"boolean","isStatic":false,"klass":"java.io.File"},
"iHash":"1243171897","date":"170309809...","uuid":"6b4..."}
```,
  caption: "Пример получаемой трассы"
) <dynamic_trace_example>


== Пакет storage

Для хранения результатов поиска проекта и извлечения из них трасс использована SQLite. Так как это локальная БД, она позволяет легко создавать переносимые базы данных в виде файла для различных запусков инструмента. Это позволяет в дальнейшем использовать их как отправную точку для поиска репозиториев, дополнять уже накопленные трассы, проводить эксперименты по восстановлению с разными параметрами и делиться накопленными данными. Также она не требует специального окружения и может создаваться с использованием соответствующей библиотеки. Работа с БД реализована с помощью ktorm.

Сама БД состоит из нескольких таблиц:
- Repository. Таблица используется для хранения уже полученных репозиториев и краткой информации о них. Содержит в себе следующие поля:
  - name -- название репозитория
  - namespace -- иначе groupId, в случае если проект -- библиотека из Maven
  - version -- версия
  - author -- автор
  - locator -- локация репозитория, для GitHub это URL, а для Maven -- PURL
  - source -- источник, т.е. GitHub или Maven
  - date -- дата получения репозитория
- Method. Для работы инструмента восстановления предварительно требуется подать на вход все возможные названия переходов. Их можно собрать, обойдя все трассы, но чтобы это избежать предлагается хранить все найденные методы. Таблица содержит следующие поля:
  - name -- название
  - class -- класс
  - args -- типы аргументов в их оригинальной последовательности
  - return_type -- возвращаемый тип
  - is_static -- является ли метод статическим. Модификаторы доступа нас не интересуют, так как без использования reflection вызвать можно только публичные методы библиотеки.
- Trace. Данная таблица предназначена для хранения трасс и содержит в себе следующие поля:
  - trace - трасса из вызовов в JSON формате. На самом деле подобное решение идет вопреки с нормализацией БД и является неоптимальным, можно хранить исключительно идентификаторы методов из таблицы Method. Но в рамках работы выбор сделан в пользу удобства просмотра накопленных трасс без средств обработки, а потенциальный объем данных в БД позволяет пренебречь нормальной формой. 
  - class - класс, для которого собрана трасса
  - count - количество найденных идентичных трасс

== Пакет inference

После извлечения и сохранения определенного количество трасс появляется возможность восстановить поведенческую модель библиотеки. Процесс восстановления происходит в несколько этапов:
+ Выбор трасс и их предобработка в  соответствии с режимом работы. Это могут быть трассы полученные только статическим или динамическим путем, а также статически полученные трассы, частично совпадающие с динамическими. Если был установлен порог погрешности, то также выполняется фильтрация трасс, которые будут использованы для восстановления
+ Формируется входной файл для фреймворка MINT, состоящий из предварительно указанных названий переходов и трасс
+ Применяется алгоритм k-tail, реализованный в рамках MINT. Результатом является граф в DOT формате
+ Определяются конечные состояния, т.е. из которых отсутствуют любые переходы.
+ Выполняется объединение конечных состояний. Для этого выбирается основное конечное состояние и у переходов, ведущих в остальные конечные состояния, подменяется точка назначения на основное. Лишние конечные состояния удаляются. Как уже было описано в разделе, посвященному проектированию, это действие не несет какой либо потери информации
+ Выполняется трансляции DOT графа в JSON для унификации формата хранения и какой-либо потенциальной работы с ним (например, интерактивное редактирование). При этом DOT остается в качестве артефакта работы инструмента

Для восстановления можно управлять параметром k, влияющим на длину минимально совпадающих частей трассы для слияния, а также выбирать другие предоставляемые mintframework стратегии восстановления и алгоритмы классификации.  

== Реализованный инструмент

В результате разработан инструмент, реализующий подход автоматического извлечения поведенческих моделей библиотек из доступных программных репозиториев. В разработке учтена возможность использования модулей независимо друг от друга, а также предоставлены возможности настройки используемого окружения и библиотек. Исходный код проекта доступен в репозитории LibMiner#footnote("https://github.com/kechinvv/LibMiner") на GitHub.